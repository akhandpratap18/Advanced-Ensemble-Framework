{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "233055ad",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c2b7866",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_memory_dumps(dump_dir, target_size=(64, 64)):\n",
    "    \"\"\"\n",
    "    Load memory dump images from directory structure and resize to fixed dimensions.\n",
    "    \n",
    "    Args:\n",
    "        dump_dir: Directory containing class subdirectories\n",
    "        target_size: Tuple of (height, width) for resizing images\n",
    "    \"\"\"\n",
    "    if not os.path.exists(dump_dir):\n",
    "        raise FileNotFoundError(f\"Memory dump directory not found: {dump_dir}\")\n",
    "    \n",
    "    # Get all subdirectories (classes)\n",
    "    class_dirs = [d for d in os.listdir(dump_dir) if os.path.isdir(os.path.join(dump_dir, d))]\n",
    "    \n",
    "    if not class_dirs:\n",
    "        raise ValueError(f\"No class directories found in {dump_dir}\")\n",
    "    \n",
    "    # Create label mapping\n",
    "    label_to_idx = {label: idx for idx, label in enumerate(sorted(class_dirs))}\n",
    "    print(f\"Found classes: {label_to_idx}\")\n",
    "    \n",
    "    # Lists to store images and labels\n",
    "    images = []\n",
    "    labels = []\n",
    "    \n",
    "    # Process each class directory\n",
    "    for class_dir in class_dirs:\n",
    "        class_path = os.path.join(dump_dir, class_dir)\n",
    "        label = label_to_idx[class_dir]\n",
    "        \n",
    "        # Get all image files in the directory\n",
    "        image_files = [f for f in os.listdir(class_path) if f.endswith(('.png'))]\n",
    "        \n",
    "        print(f\"Processing {len(image_files)} images from class {class_dir}\")\n",
    "        \n",
    "        for img_file in image_files:\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "            try:\n",
    "                # Read raw binary data\n",
    "                data = np.fromfile(img_path, dtype=np.uint8)\n",
    "                \n",
    "                # Convert to square image\n",
    "                width = int(np.sqrt(len(data)))\n",
    "                if width * width == len(data):\n",
    "                    img = data.reshape(width, width)\n",
    "                else:\n",
    "                    # Pad to nearest square\n",
    "                    square_size = int(np.ceil(np.sqrt(len(data))))\n",
    "                    img = np.zeros((square_size, square_size), dtype=np.uint8)\n",
    "                    img.flat[:len(data)] = data\n",
    "                \n",
    "                # Resize to target size using cv2\n",
    "                img_resized = cv2.resize(img, target_size, interpolation=cv2.INTER_AREA)\n",
    "                \n",
    "                # Flatten the resized image\n",
    "                img_flat = img_resized.ravel()\n",
    "                \n",
    "                images.append(img_flat)\n",
    "                labels.append(label)\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {img_path}: {str(e)}\")\n",
    "                continue\n",
    "    \n",
    "    if not images:\n",
    "        raise ValueError(\"No valid images found in any class directory\")\n",
    "    \n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(images)\n",
    "    y = np.array(labels)\n",
    "    \n",
    "    print(f\"Loaded {len(X)} total images across {len(class_dirs)} classes\")\n",
    "    print(f\"Feature vector shape: {X.shape}\")\n",
    "    \n",
    "    return X, y, label_to_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4de6d230",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def apply_pca(X, variance_threshold=0.9):\n",
    "    \"\"\"\n",
    "    Apply PCA while preserving specified variance threshold.\n",
    "    \n",
    "    Args:\n",
    "        X: Input features (n_samples, n_features)\n",
    "        variance_threshold: Minimum variance to preserve (default: 0.9 for 90%).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (transformed data, fitted PCA object, scaler object)\n",
    "    \"\"\"\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Initialize PCA without specifying number of components\n",
    "    pca = PCA()\n",
    "    pca.fit(X_scaled)\n",
    "    \n",
    "    # Calculate cumulative variance ratio\n",
    "    cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "    \n",
    "    # Find number of components that preserve desired variance\n",
    "    n_components = np.argmax(cumulative_variance_ratio >= variance_threshold) + 1\n",
    "    \n",
    "    # Fit PCA with selected number of components\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_transformed = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    return X_transformed, pca, scaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b731d6ac",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def plot_variance_explained(pca, output_path):\n",
    "    \"\"\"\n",
    "    Plot cumulative variance explained by principal components.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Cumulative variance plot\n",
    "    cumulative_variance_ratio = np.cumsum(pca.explained_variance_ratio_)\n",
    "    plt.plot(range(1, len(cumulative_variance_ratio) + 1), \n",
    "            cumulative_variance_ratio, \n",
    "            'bo-', label='Cumulative Explained Variance')\n",
    "    \n",
    "    # 90% threshold line\n",
    "    plt.axhline(y=0.9, color='r', linestyle='--', label='90% Variance Threshold')\n",
    "    \n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "    plt.title('PCA Explained Variance')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plt.savefig(output_path)\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dfed4bf",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def main():\n",
    "    # Specify paths\n",
    "    dump_dir = r\"C:\\Users\\bacaup\\AEF\\TRAINING\"  # Replace with your memory dumps directory\n",
    "    output_dir = \"pca_results\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load memory dumps with fixed size\n",
    "    print(\"Loading memory dumps...\")\n",
    "    X, y, label_mapping = load_memory_dumps(dump_dir, target_size=(64, 64))\n",
    "    \n",
    "    # Apply PCA\n",
    "    print(\"\\nApplying PCA...\")\n",
    "    X_transformed, pca, scaler = apply_pca(X, variance_threshold=0.9)\n",
    "    \n",
    "    print(f\"\\nOriginal feature dimension: {X.shape[1]}\")\n",
    "    print(f\"Reduced feature dimension: {X_transformed.shape[1]}\")\n",
    "    print(f\"Variance preserved: {sum(pca.explained_variance_ratio_):.4f}\")\n",
    "    \n",
    "    # Plot variance explained\n",
    "    plot_path = os.path.join(output_dir, \"pca_variance_explained.png\")\n",
    "    plot_variance_explained(pca, plot_path)\n",
    "    print(f\"\\nVariance plot saved to: {plot_path}\")\n",
    "    \n",
    "    # Decode the labels back to the original class names\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(list(label_mapping.keys()))\n",
    "    decoded_labels = label_encoder.inverse_transform(y)\n",
    "    \n",
    "    # Save transformed data with decoded labels\n",
    "    output_data = pd.DataFrame(X_transformed)\n",
    "    output_data.columns = [f'PC_{i+1}' for i in range(X_transformed.shape[1])]\n",
    "    output_data['label'] = decoded_labels  # Add decoded labels back\n",
    "    \n",
    "    output_path = os.path.join(output_dir, \"pca_transformed_data.csv\")\n",
    "    output_data.to_csv(output_path, index=False)\n",
    "    print(f\"Transformed data saved to: {output_path}\")\n",
    "    \n",
    "    # Save PCA components and explained variance\n",
    "    components_df = pd.DataFrame(\n",
    "        pca.components_,\n",
    "        columns=[f'feature_{i}' for i in range(X.shape[1])],\n",
    "        index=[f'PC_{i+1}' for i in range(X_transformed.shape[1])]\n",
    "    )\n",
    "    components_path = os.path.join(output_dir, \"pca_components.csv\")\n",
    "    components_df.to_csv(components_path)\n",
    "    \n",
    "    # Save explained variance ratios\n",
    "    variance_df = pd.DataFrame({\n",
    "        'component': [f'PC_{i+1}' for i in range(len(pca.explained_variance_ratio_))],\n",
    "        'explained_variance_ratio': pca.explained_variance_ratio_,\n",
    "        'cumulative_variance_ratio': np.cumsum(pca.explained_variance_ratio_)\n",
    "    })\n",
    "    variance_path = os.path.join(output_dir, \"explained_variance.csv\")\n",
    "    variance_df.to_csv(variance_path, index=False)\n",
    "    \n",
    "    print(f\"\\nComponent details saved to: {components_path}\")\n",
    "    print(f\"Variance details saved to: {variance_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f72f77b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading memory dumps...\n",
      "Found classes: {'.ipynb_checkpoints': 0, 'Adposhel': 1, 'Allaple': 2, 'Amonetize': 3, 'AutoRun': 4, 'BrowseFox': 5, 'Dinwod': 6, 'InstallCore': 7, 'MultiPlug': 8, 'Other': 9, 'VBA': 10, 'Vilsel': 11}\n",
      "Processing 0 images from class .ipynb_checkpoints\n",
      "Processing 364 images from class Adposhel\n",
      "Processing 349 images from class Allaple\n",
      "Processing 349 images from class Amonetize\n",
      "Processing 158 images from class AutoRun\n",
      "Processing 152 images from class BrowseFox\n",
      "Processing 98 images from class Dinwod\n",
      "Processing 376 images from class InstallCore\n",
      "Processing 390 images from class MultiPlug\n",
      "Processing 487 images from class Other\n",
      "Processing 399 images from class VBA\n",
      "Processing 311 images from class Vilsel\n",
      "Loaded 3433 total images across 12 classes\n",
      "Feature vector shape: (3433, 4096)\n",
      "\n",
      "Applying PCA...\n",
      "\n",
      "Original feature dimension: 4096\n",
      "Reduced feature dimension: 1793\n",
      "Variance preserved: 0.8973\n",
      "\n",
      "Variance plot saved to: pca_results\\pca_variance_explained.png\n",
      "Transformed data saved to: pca_results\\pca_transformed_data.csv\n",
      "\n",
      "Component details saved to: pca_results\\pca_components.csv\n",
      "Variance details saved to: pca_results\\explained_variance.csv\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "770104ba-0ac1-48b8-a61b-a0730db13fb9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
